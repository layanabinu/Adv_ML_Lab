{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (15, 4)\n",
      "Subset of the dataset:\n",
      "      Country   Age   Salary Purchased\n",
      "0      India  34.0  92000.0       Yes\n",
      "1  Sri lanka  22.0  25000.0       Yes\n",
      "2      China  31.0  74000.0       Yes\n",
      "3  Sri lanka  29.0      NaN        No\n",
      "4      China  55.0  98000.0       Yes\n",
      "Subset of Features X:\n",
      " [['India' 34.0 92000.0]\n",
      " ['Sri lanka' 22.0 25000.0]\n",
      " ['China' 31.0 74000.0]\n",
      " ['Sri lanka' 29.0 nan]\n",
      " ['China' 55.0 98000.0]]\n",
      "Subset of Target Y:\n",
      " ['Yes' 'Yes' 'Yes' 'No' 'Yes']\n",
      "Subset of Features X after encoding:\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0.]]\n",
      "Subset of Features X after handling missing values:\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0.]]\n",
      "X_train shape: (12, 31), X_test shape: (3, 31)\n",
      "Subset of Features X_train after scaling:\n",
      " [[-0.57735027  1.         -0.57735027 -0.30151134  0.         -0.30151134\n",
      "  -0.30151134  0.         -0.30151134 -0.30151134 -0.4472136  -0.30151134\n",
      "  -0.30151134  3.31662479  0.         -0.30151134 -0.30151134 -0.30151134\n",
      "  -0.30151134  0.         -0.4472136   0.         -0.30151134 -0.30151134\n",
      "  -0.30151134 -0.30151134  3.31662479  0.         -0.30151134 -0.30151134\n",
      "  -0.30151134]\n",
      " [-0.57735027 -1.          1.73205081 -0.30151134  0.         -0.30151134\n",
      "  -0.30151134  0.         -0.30151134 -0.30151134  2.23606798 -0.30151134\n",
      "  -0.30151134 -0.30151134  0.         -0.30151134 -0.30151134 -0.30151134\n",
      "   3.31662479  0.         -0.4472136   0.         -0.30151134 -0.30151134\n",
      "  -0.30151134 -0.30151134 -0.30151134  0.         -0.30151134 -0.30151134\n",
      "  -0.30151134]\n",
      " [ 1.73205081 -1.         -0.57735027 -0.30151134  0.         -0.30151134\n",
      "  -0.30151134  0.         -0.30151134 -0.30151134 -0.4472136  -0.30151134\n",
      "  -0.30151134 -0.30151134  0.          3.31662479 -0.30151134 -0.30151134\n",
      "  -0.30151134  0.         -0.4472136   0.         -0.30151134 -0.30151134\n",
      "  -0.30151134 -0.30151134 -0.30151134  0.         -0.30151134  3.31662479\n",
      "  -0.30151134]\n",
      " [ 1.73205081 -1.         -0.57735027 -0.30151134  0.         -0.30151134\n",
      "  -0.30151134  0.         -0.30151134  3.31662479 -0.4472136  -0.30151134\n",
      "  -0.30151134 -0.30151134  0.         -0.30151134 -0.30151134 -0.30151134\n",
      "  -0.30151134  0.         -0.4472136   0.         -0.30151134 -0.30151134\n",
      "  -0.30151134  3.31662479 -0.30151134  0.         -0.30151134 -0.30151134\n",
      "  -0.30151134]\n",
      " [-0.57735027  1.         -0.57735027 -0.30151134  0.         -0.30151134\n",
      "  -0.30151134  0.         -0.30151134 -0.30151134 -0.4472136  -0.30151134\n",
      "   3.31662479 -0.30151134  0.         -0.30151134 -0.30151134 -0.30151134\n",
      "  -0.30151134  0.         -0.4472136   0.         -0.30151134 -0.30151134\n",
      "   3.31662479 -0.30151134 -0.30151134  0.         -0.30151134 -0.30151134\n",
      "  -0.30151134]]\n",
      "Subset of Features X_test after scaling:\n",
      " [[-0.57735027 -1.          1.73205081 -0.30151134  1.         -0.30151134\n",
      "  -0.30151134  0.         -0.30151134 -0.30151134 -0.4472136  -0.30151134\n",
      "  -0.30151134 -0.30151134  0.         -0.30151134 -0.30151134 -0.30151134\n",
      "  -0.30151134  1.         -0.4472136   0.         -0.30151134 -0.30151134\n",
      "  -0.30151134 -0.30151134 -0.30151134  0.         -0.30151134 -0.30151134\n",
      "  -0.30151134]\n",
      " [-0.57735027 -1.          1.73205081 -0.30151134  0.         -0.30151134\n",
      "  -0.30151134  1.         -0.30151134 -0.30151134 -0.4472136  -0.30151134\n",
      "  -0.30151134 -0.30151134  0.         -0.30151134 -0.30151134 -0.30151134\n",
      "  -0.30151134  0.         -0.4472136   1.         -0.30151134 -0.30151134\n",
      "  -0.30151134 -0.30151134 -0.30151134  0.         -0.30151134 -0.30151134\n",
      "  -0.30151134]\n",
      " [ 1.73205081 -1.         -0.57735027 -0.30151134  0.         -0.30151134\n",
      "  -0.30151134  0.         -0.30151134 -0.30151134 -0.4472136  -0.30151134\n",
      "  -0.30151134 -0.30151134  1.         -0.30151134 -0.30151134 -0.30151134\n",
      "  -0.30151134  0.         -0.4472136   0.         -0.30151134 -0.30151134\n",
      "  -0.30151134 -0.30151134 -0.30151134  1.         -0.30151134 -0.30151134\n",
      "  -0.30151134]]\n",
      "Data preprocessing completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load the dataset\n",
    "dataset = pd.read_csv(\"C:/vscodefolder/AML_Lab/Datasets/Data.csv\")\n",
    "print(f'Dataset shape: {dataset.shape}')\n",
    "print('Subset of the dataset:\\n', dataset.head())\n",
    "\n",
    "# 2. Separate the dependent and independent variables\n",
    "X = dataset.iloc[:, :-1].values  # Features (including categorical)\n",
    "Y = dataset.iloc[:, -1].values   # Target\n",
    "print('Subset of Features X:\\n', X[:5])\n",
    "print('Subset of Target Y:\\n', Y[:5])\n",
    "\n",
    "# 3. Handle categorical data first\n",
    "X[:, 0] = LabelEncoder().fit_transform(X[:, 0])\n",
    "X = OneHotEncoder(sparse_output=False).fit_transform(X)\n",
    "print('Subset of Features X after encoding:\\n', X[:5])\n",
    "\n",
    "# Convert back to DataFrame to handle missing values in numerical columns\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# 4. Handle missing values by replacing with mean or interpolation\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "df.interpolate(inplace=True)\n",
    "X = df.values\n",
    "print('Subset of Features X after handling missing values:\\n', X[:5])\n",
    "\n",
    "# 5. Split the dataset into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "print(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')\n",
    "\n",
    "# 6. Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print('Subset of Features X_train after scaling:\\n', X_train[:5])\n",
    "print('Subset of Features X_test after scaling:\\n', X_test[:5])\n",
    "\n",
    "print('Data preprocessing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (15, 4)\n",
      "Subset of the dataset:\n",
      "      Country   Age   Salary Purchased\n",
      "0      India  34.0  92000.0       Yes\n",
      "1  Sri lanka  22.0  25000.0       Yes\n",
      "2      China  31.0  74000.0       Yes\n",
      "3  Sri lanka  29.0      NaN        No\n",
      "4      China  55.0  98000.0       Yes\n",
      "Missing values in dataset before handling:\n",
      " Country      0\n",
      "Age          1\n",
      "Salary       1\n",
      "Purchased    0\n",
      "dtype: int64\n",
      "Missing values in dataset after handling:\n",
      " Country      0\n",
      "Age          0\n",
      "Salary       0\n",
      "Purchased    0\n",
      "dtype: int64\n",
      "Subset of Features X after encoding:\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0.]]\n",
      "X_train shape: (12, 28), X_test shape: (3, 28)\n",
      "Subset of Features X_train after scaling:\n",
      " [[ 1.         -0.57735027  0.         -0.30151134 -0.30151134  0.\n",
      "  -0.30151134 -0.30151134 -0.30151134 -0.4472136  -0.30151134 -0.30151134\n",
      "   3.31662479  0.         -0.30151134 -0.30151134  0.         -0.4472136\n",
      "   0.         -0.30151134 -0.30151134 -0.30151134 -0.30151134 -0.30151134\n",
      "   3.31662479  0.         -0.30151134 -0.30151134]]\n",
      "Subset of Features X_test after scaling:\n",
      " [[-1.          1.73205081  1.         -0.30151134 -0.30151134  0.\n",
      "  -0.30151134 -0.30151134 -0.30151134 -0.4472136  -0.30151134 -0.30151134\n",
      "  -0.30151134  0.         -0.30151134 -0.30151134  1.         -0.4472136\n",
      "   0.         -0.30151134 -0.30151134 -0.30151134 -0.30151134 -0.30151134\n",
      "  -0.30151134  0.         -0.30151134 -0.30151134]]\n",
      "Data preprocessing completed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load the dataset\n",
    "dataset = pd.read_csv(\"C:/vscodefolder/AML_Lab/Datasets/Data.csv\")\n",
    "print(f'Dataset shape: {dataset.shape}')\n",
    "print('Subset of the dataset:\\n', dataset.head())\n",
    "\n",
    "# 2. Check for missing values\n",
    "print('Missing values in dataset before handling:\\n', dataset.isnull().sum())\n",
    "\n",
    "# 3. Handle missing values\n",
    "# Fill missing 'Age' with the median age\n",
    "dataset['Age'] = dataset['Age'].fillna(dataset['Age'].median())\n",
    "# Fill missing 'Salary' with the median salary\n",
    "dataset['Salary'] = dataset['Salary'].fillna(dataset['Salary'].median())\n",
    "\n",
    "# Check for missing values again\n",
    "print('Missing values in dataset after handling:\\n', dataset.isnull().sum())\n",
    "\n",
    "# 4. Encode categorical data\n",
    "# Encode 'Country' using OneHotEncoder\n",
    "X = dataset[['Country', 'Age', 'Salary']].values\n",
    "Y = dataset['Purchased'].values\n",
    "\n",
    "# Use LabelEncoder for 'Purchased' target variable\n",
    "labelencoder_Y = LabelEncoder()\n",
    "Y = labelencoder_Y.fit_transform(Y)\n",
    "\n",
    "# OneHotEncoding for 'Country' feature\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0])\n",
    "onehotencoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' to avoid multicollinearity\n",
    "X = onehotencoder.fit_transform(X)\n",
    "print('Subset of Features X after encoding:\\n', X[:1])\n",
    "\n",
    "# 5. Split the dataset into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "print(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')\n",
    "\n",
    "# 6. Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print('Subset of Features X_train after scaling:\\n', X_train[:1])\n",
    "print('Subset of Features X_test after scaling:\\n', X_test[:1])\n",
    "\n",
    "print('Data preprocessing completed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
